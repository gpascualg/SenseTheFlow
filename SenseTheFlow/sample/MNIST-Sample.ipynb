{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Sample\n",
    "\n",
    "This notebook creates a sample model for MNIST using Tensorflow and SenseTheFlow.\n",
    "\n",
    "As a sample, it is an overkill that will use Keras Resnet50 implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from SenseTheFlow.experiment import Experiment, Mode, ExperimentOutput, keras_weight_path, ExperimentHook, Hookpoint\n",
    "from SenseTheFlow.layers import utils\n",
    "from SenseTheFlow.models.resnet import ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "\n",
    "Models are now sub-classes from `tf.keras.Model`. This is to make user code and the library itself compatible with future Tensorflow 2.0 usage.\n",
    "\n",
    "Such models should:\n",
    "* Define layers in its `__init__(self, mode, params)` constructor. The easiest way to do so would be by using either `tf.keras.layers` or `keras.layers`.\n",
    "* Call layers in its `call(self, x, y, mode, params)` method.\n",
    "\n",
    "The `call` method must return a `ExperimentOutput` instance. Its property `loss` must be different from `None`, either a tensor or a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(tf.keras.Model):\n",
    "    def __init__(self, mode, params):\n",
    "        # Always call `super` constructor\n",
    "        super(MNIST, self).__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu', padding='same', data_format=params['data_format'])\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same', data_format=params['data_format'])\n",
    "        \n",
    "        # Example on how to add L2 to a layer, no need to manually touch the loss term, it will be automatically used\n",
    "        # I use 3 filters as resnet requires 3 channels\n",
    "        self.conv3 = tf.keras.layers.Conv2D(filters=3, kernel_size=3, activation='relu', padding='same', data_format=params['data_format'], \n",
    "                                           kernel_regularizer=tf.keras.regularizers.l2(0.001)) \n",
    "        \n",
    "        # Make sure keras uses our data_format\n",
    "        tf.keras.backend.set_image_data_format(params['data_format'])\n",
    "        \n",
    "        # If you intend to train this resnet, you can leave it as is\n",
    "        # Otherwise, make sure to set \"trainable_bn=False, l2=None\"\n",
    "        self.resnet = ResNet50(params['data_format'], trainable_bn=mode==Mode.TRAIN, l2=0.001)\n",
    "        \n",
    "        # Note this is a top-less resnet, it doesn't have pooling/dense layers\n",
    "        self.pool = tf.keras.layers.GlobalAveragePooling2D(data_format=params['data_format'])\n",
    "        self.dense = tf.keras.layers.Dense(10)\n",
    "        \n",
    "        # Optimizer and other params\n",
    "        self.params = params\n",
    "        self.mode = mode\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "        \n",
    "        # Let's keep an average of the loss\n",
    "        self.loss_avg = tf.keras.metrics.Mean()\n",
    "        \n",
    "    def call(self, inputs, training, step):\n",
    "        if self.mode != Mode.TEST:\n",
    "            x, y = inputs\n",
    "        else:\n",
    "            x = inputs\n",
    "            \n",
    "        # Make sure the image is in the correct data format, we get it in channels_last \n",
    "        x = utils.to_data_format(x, 'channels_last', params['data_format'])\n",
    "            \n",
    "        # Call layers\n",
    "        outputs = self.conv1(x)\n",
    "        outputs = self.conv2(outputs)\n",
    "        outputs = self.conv3(outputs)\n",
    "        outputs = self.resnet(outputs)\n",
    "        outputs = self.pool(outputs)\n",
    "        logits = self.dense(outputs)\n",
    "        \n",
    "        # When `mode == Mode.TEST`, `y` will be None\n",
    "        loss = 0.0\n",
    "        if self.mode != Mode.TEST:\n",
    "            # Compute loss\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            \n",
    "            # Update average\n",
    "            self.loss_avg.update_state(loss)\n",
    "            \n",
    "        # Summary examples\n",
    "        if tf.equal(tf.math.floormod(step, self.params['summary_steps']), 0):\n",
    "            with tf.device(\"cpu:0\"):\n",
    "                tf.summary.scalar('loss', loss, step=step)\n",
    "                \n",
    "                # Include the loss average\n",
    "                tf.summary.scalar('loss/avg', self.loss_avg.result(), step=step)\n",
    "                \n",
    "                # We should reset this here, but tensorflow won't like it and it will be\n",
    "                #  ignored. So, we are doing it later and more convoluted...\n",
    "                # self.loss_avg.reset_states()\n",
    "        \n",
    "        # `call` must return `ExperimentOutput`, and it must be constructed as a \"key=value\" object\n",
    "        return ExperimentOutput(\n",
    "            outputs={\n",
    "                'probabilities': tf.nn.softmax(logits),\n",
    "                'number': tf.argmax(logits)\n",
    "            },\n",
    "            loss=loss\n",
    "        )\n",
    "    \n",
    "    # If this method exists, it is automatically called once an epoch is done\n",
    "    # It is specially useful during evaluations, as it allows to register the\n",
    "    #  last accumulate value on metrics\n",
    "    def on_epoch(self, step):\n",
    "        tf.summary.scalar('loss/avg', self.loss_avg.result(), step=step)\n",
    "\n",
    "    # This will be manually hooked later, basically resets all metrics to its\n",
    "    #  initial state. It is usually done at the same moment as when the summaries are\n",
    "    #  saved\n",
    "    def reset(self, *args, **kwargs):\n",
    "        self.loss_avg.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'model_name': 'mnist-test',\n",
    "    'learning_rate': 3e-4,\n",
    "    'batch_size': 128,\n",
    "    'summary_steps': 1000,\n",
    "    'data_format': 'channels_first'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the data pipeline\n",
    "\n",
    "It can be done in multiple ways, the only restriction is for your `dataset_fn` to return a `tf.Dataset` instance. Here, we will use `Keras` to directly obtain images and labels, but one could read from disk or whatever other option.\n",
    "\n",
    "Here using a generator might not be the best idea (performance-wise), as we have all data already loaded into RAM. However, the general case will be generators, thus it is the pipeline used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_generator():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    for image, label in zip(x_train, y_train):\n",
    "        # Here we simply yield two values, but you could easily construct other returns.\n",
    "        # More examples are provided below\n",
    "        yield image[..., np.newaxis], label\n",
    "        \n",
    "# `dataset_fn` always receives the run `mode`.\n",
    "def dataset_fn(mode, params):\n",
    "    # Create a dataset from a geneator\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator=mnist_generator, \n",
    "        output_types=(tf.float32, tf.int32),\n",
    "        output_shapes=([28, 28, 1], None)\n",
    "    )\n",
    "\n",
    "    # Batch inputs, make sure all batches are equal-sized (drop_remainder=True)\n",
    "    dataset = dataset.batch(params['batch_size'], drop_remainder=True)\n",
    "    # Prefetch some batches\n",
    "    dataset = dataset.prefetch(8)\n",
    "    # Repeat for 100 epochs, leave blank to repeat indefinately\n",
    "    dataset = dataset.repeat(100)\n",
    "    # Return the dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 2 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# The new way to tell TF not to use all GPUs but only the exact amount needed\n",
    "# The next cell will attempt to use GPU space if executed, so do it now\n",
    "#  otherwise it would be fine to do it just before running the model\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(5, shape=(), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Errors in the above functions are particularly hard to debug once in the network\n",
    "# you can manually inspect everything works like this\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    dataset = dataset_fn(Mode.TRAIN, params)\n",
    "    for x, y in dataset:\n",
    "        for batch in range(params['batch_size']):\n",
    "            print(y[batch])\n",
    "            plt.imshow(x[batch, ..., 0])\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 24 16:59:36 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.43       Driver Version: 418.43       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN Xp            Off  | 00000000:03:00.0 Off |                  N/A |\n",
      "| 27%   44C    P2    62W / 250W |    157MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN X (Pascal)    Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 54%   68C    P2    61W / 250W |    147MiB / 12192MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the experiment\n",
    "\n",
    "Experiments are executed in two steps:\n",
    "\n",
    "1. Experiment discovery: That is, given the experiment name (or model name) find the corresponding saved checkpoint (if any). This process might not be completed immediately due to:\n",
    "  * Remote execution, when the model has to be fetched from the main data-storage\n",
    "  * Local jupyter execution, when the model is selected from the drop-down\n",
    "  \n",
    "2. Experiment execution: Calling train/eval/test and waiting for the results. SenseTheFlow is asynchronous by default, which means that all executions are done in a background thread. Some mechanisms are provided to block until certain events\n",
    "  * `wait_ready` might be used to wait until the experiment actually starts running (waits until tensorflow has setup the GPU memory and starts iterating)\n",
    "  * `wait_for` can be used to wait for a certain number of steps or seconds\n",
    "  * `wait` can be used to block until the experiment completely finishes\n",
    "  \n",
    "Alternatively, asynchronous execution can be turned off by specifying `sync=True` to any of the calls above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def once_discovered(experiment):\n",
    "    # There are two special methods that might be called pre/post initialization of the network\n",
    "    #  * Pre-initialization is useful to manually load another checkpoint (aside from the one of this same model,\n",
    "    #    which is loaded automatically)\n",
    "    #  * Post-initialization is used to load some keras-specific weights, as is the case with \n",
    "    #    resnet and such models\n",
    "    \n",
    "    def pre_initialize_fn(experiment, model, mode, ckpt):\n",
    "        # Also, you can add here some special hooks that, for example, call methods of the model itself\n",
    "        # We reset all metrics with the following hook\n",
    "        experiment.add_hook(Hookpoint.LOOP, ExperimentHook('reset', model.params['summary_steps'], model.reset, concurrent=False))\n",
    "        \n",
    "        # We can manually load another chekpoint by manually doing so\n",
    "        # NOTE that SenseTheFlow will still load the latest checkpoint after, an schematic is provided below\n",
    "        \"\"\"\n",
    "        ckpt = tf.train.Checkpoint(net=model)\n",
    "        warm_start = tf.train.latest_checkpoint('/path to some othe model whatsoever')\n",
    "        print('Warm starting from {}'.format(warm_start))\n",
    "        restore = ckpt.restore(warm_start)\n",
    "        return restore, restore.assert_existing_objects_matched # See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint for assert_existing_objects_matched\n",
    "        \"\"\"\n",
    "    \n",
    "    def post_initialize_fn(experiment, model, mode, last_checkpoint):\n",
    "        # In case the model has an existing checkpoint, it will have already been loaded by now\n",
    "        #  which means Resnet is already loaded\n",
    "        # If we allowed the resnet to be modified it is of high importance to return here, otherwise\n",
    "        #  we will load over the trained weights the original resnet\n",
    "        if last_checkpoint:\n",
    "            print('Skip RESNET load')\n",
    "            return\n",
    "        \n",
    "        # Otherwise, load the weights\n",
    "        print('Loading resnet')\n",
    "        model.resnet.load(keras_weight_path('resnet50'), by_name=True)\n",
    "    \n",
    "    # Call train, by default will:\n",
    "    #  * checkpoint_steps=1000, saves the model every 1000 steps\n",
    "    #  * summary_steps=100, saves summaries every 100 steps\n",
    "    #  * config=None, which means it will *not* use all gpus, but gradually expand memory\n",
    "    # By default the model is executed in a background thread, which might be a BAD idea\n",
    "    #  if you have errors and want to find them\n",
    "    #  use sync=True to block and run normally\n",
    "    print('TRAIN')\n",
    "    context = experiment.train(dataset_fn, pre_initialize_fn=pre_initialize_fn, post_initialize_fn=post_initialize_fn)\n",
    "    \n",
    "    # Wait until it actually is running\n",
    "    context.wait_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As always, you may not want debug messages\n",
    "\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78889f5d8bb4a2080f1ef27a3ac5528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Select(options=(('-', None), (' |- Create new model', -1), (' |- Remove previous and create new', -2), ('-', N…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# By default everything is run in Graph mode (ie. compiled, fast). It can be optionally executed eagerly (step by step) by using\n",
    "#tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "# SenseTheFlow is not the best friend of how keras downloads the resnet weights,\n",
    "# so we will force it here. If they already exist this will do nothing\n",
    "keras_weight_path('resnet50')\n",
    "\n",
    "# Supply the name, model CLASS, params and where to write\n",
    "experiment = Experiment(params['model_name'], MNIST, params=params, persistent_path=os.path.join('/tmp/', params['model_name']))\n",
    "# Tell it which GPU to use\n",
    "experiment.assign_gpu(1)\n",
    "# And search for existing models\n",
    "experiment.run_local(once_discovered, prepend_timestamp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waiting\n",
    "\n",
    "An experiment can be waited, in general, by calling any of `wait_ready` or `wait`. As opposed to waiting on a context, waiting on an experiment will wait for all individual runs to end (for example, wait for both train and eval to end)\n",
    "\n",
    "It is encoraged to call `wait_ready` before continuing, to ensure the context is registered.\n",
    "\n",
    "**You must first select the chekpoint in case it is prompted, otherwise this will indefinately block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.wait_ready()\n",
    "# experiment.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further using the context\n",
    "\n",
    "The training `context` above, inside `once_discovered`, can be recovered with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As it is the first and only call to train, out context is in position [0] of Mode.TRAIN\n",
    "# Further calls would be in next positions, ordered by call order always\n",
    "\n",
    "context = experiment.get_context(Mode.TRAIN)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62c1d1037284f5daded1ad4774af36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397e9ce4f50c42448265953a89aa8161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context.reattach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can do either of this:\n",
    "\n",
    "# context.save()\n",
    "# context.reattach() # Redraws the TQDM bars\n",
    "# context.stop() # Stops training\n",
    "\n",
    "# Wait until it finishes, can exit waiting by pressing stop or CTRL+C, training won't be affected.\n",
    "# context.wait()\n",
    "\n",
    "context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
