{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Sample\n",
    "\n",
    "This notebook creates a sample model for MNIST using Tensorflow and SenseTheFlow.\n",
    "\n",
    "As a sample, it is an overkill that will use Keras Resnet50 implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade git+https://github.com/gpascualg/SenseTheFlow.git@tf-2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from SenseTheFlow.experiment import Experiment, Mode, ExperimentOutput, keras_weight_path, ExperimentHook, Hookpoint, default_config\n",
    "from SenseTheFlow.layers import utils\n",
    "from SenseTheFlow.models.resnet import ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "\n",
    "Models are now sub-classes from `tf.keras.Model`. This is to make user code and the library itself compatible with future Tensorflow 2.0 usage.\n",
    "\n",
    "Such models should:\n",
    "* Define layers in its `__init__(self, mode, params)` constructor. The easiest way to do so would be by using either `tf.keras.layers` or `keras.layers`.\n",
    "* Call layers in its `call(self, x, y, mode, params)` method.\n",
    "\n",
    "The `call` method must return a `ExperimentOutput` instance. Its property `loss` must be different from `None`, either a tensor or a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(tf.keras.Model):\n",
    "    def __init__(self, mode, params):\n",
    "        # Always call `super` constructor\n",
    "        super(MNIST, self).__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu', padding='same', data_format=params['data_format'])\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same', data_format=params['data_format'])\n",
    "        \n",
    "        # Example on how to add L2 to a layer, no need to manually touch the loss term, it will be automatically used\n",
    "        # I use 3 filters as resnet requires 3 channels\n",
    "        self.conv3 = tf.keras.layers.Conv2D(filters=3, kernel_size=3, activation='relu', padding='same', data_format=params['data_format'], \n",
    "                                           kernel_regularizer=tf.keras.regularizers.l2(0.001)) \n",
    "        \n",
    "        # Make sure keras uses our data_format\n",
    "        tf.keras.backend.set_image_data_format(params['data_format'])\n",
    "        \n",
    "        # If you intend to train this resnet, you can leave it as is\n",
    "        # Otherwise, make sure to set \"trainable_bn=False, l2=None\"\n",
    "        self.resnet = ResNet50(params['data_format'], trainable_bn=mode==Mode.TRAIN, l2=0.001)\n",
    "        \n",
    "        # Note this is a top-less resnet, it doesn't have pooling/dense layers\n",
    "        self.pool = tf.keras.layers.GlobalAveragePooling2D(data_format=params['data_format'])\n",
    "        self.dense = tf.keras.layers.Dense(10)\n",
    "        \n",
    "        # Optimizer and other params\n",
    "        self.params = params\n",
    "        self.mode = mode\n",
    "        \n",
    "        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        # OPTIMIZER NO LONGER HERE\n",
    "        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        \n",
    "        # Let's keep an average of the loss\n",
    "        # All metrics are automatically reset on every epoch, even if you don't do it yourself\n",
    "        self.loss_avg = tf.keras.metrics.Mean()\n",
    "        \n",
    "    def call(self, inputs, training, step):\n",
    "        if self.mode != Mode.TEST:\n",
    "            x, y = inputs\n",
    "        else:\n",
    "            x = inputs\n",
    "            \n",
    "        # Make sure the image is in the correct data format, we get it in channels_last \n",
    "        x = utils.to_data_format(x, 'channels_last', params['data_format'])\n",
    "            \n",
    "        # Call layers\n",
    "        outputs = self.conv1(x)\n",
    "        outputs = self.conv2(outputs)\n",
    "        outputs = self.conv3(outputs)\n",
    "        outputs = self.resnet(outputs)\n",
    "        outputs = self.pool(outputs)\n",
    "        logits = self.dense(outputs)\n",
    "        \n",
    "        # When `mode == Mode.TEST`, `y` will be None\n",
    "        loss = 0.0\n",
    "        if self.mode != Mode.TEST:\n",
    "            # Compute loss\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            \n",
    "            # Update average\n",
    "            self.loss_avg.update_state(loss)\n",
    "            \n",
    "        # Summary examples\n",
    "        if tf.equal(tf.math.floormod(step, self.params['summary_steps']), 0):\n",
    "            with tf.device(\"cpu:0\"):\n",
    "                tf.summary.scalar('loss', loss, step=step)\n",
    "                \n",
    "                # Include the loss average\n",
    "                tf.summary.scalar('loss/avg', self.loss_avg.result(), step=step)\n",
    "                \n",
    "                # We should reset this here, but tensorflow won't like it and it will be\n",
    "                #  ignored. So, we are doing it later and more convoluted...\n",
    "                # self.loss_avg.reset_states()\n",
    "        \n",
    "        # `call` must return `ExperimentOutput`, and it must be constructed as a \"key=value\" object\n",
    "        return ExperimentOutput(\n",
    "            outputs={\n",
    "                'probabilities': tf.nn.softmax(logits),\n",
    "                'number': tf.argmax(logits)\n",
    "            },\n",
    "            loss=loss\n",
    "        )\n",
    "    \n",
    "    # If this method exists, it is automatically called once an epoch is done\n",
    "    # It is specially useful during evaluations, as it allows to register the\n",
    "    #  last accumulated value of metrics\n",
    "    def on_epoch(self, step):\n",
    "        tf.summary.scalar('loss/avg', self.loss_avg.result(), step=step)\n",
    "\n",
    "    # This will be manually hooked later, basically resets all metrics to its\n",
    "    #  initial state. It is usually done at the same moment as when the summaries are\n",
    "    #  saved\n",
    "    def reset(self, *args, **kwargs):\n",
    "        self.loss_avg.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'model_name': 'mnist-test',\n",
    "    'learning_rate': 3e-4,\n",
    "    'batch_size': 128,\n",
    "    'summary_steps': 1000,\n",
    "    'data_format': 'channels_last'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the data pipeline\n",
    "\n",
    "It can be done in multiple ways, the only restriction is for your `dataset_fn` to return a `tf.Dataset` instance. Here, we will use `Keras` to directly obtain images and labels, but one could read from disk or whatever other option.\n",
    "\n",
    "Here using a generator might not be the best idea (performance-wise), as we have all data already loaded into RAM. However, the general case will be generators, thus it is the pipeline used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_generator(mode):\n",
    "    data_train, data_test = tf.keras.datasets.mnist.load_data()\n",
    "    x, y = data_train if mode == Mode.TRAIN else data_test\n",
    "    \n",
    "    for image, label in zip(x, y):\n",
    "        # Here we simply yield two values, but you could easily construct other returns.\n",
    "        # More examples are provided below\n",
    "        yield image[..., np.newaxis], label\n",
    "        \n",
    "# `dataset_fn` always receives the run `mode`.\n",
    "def dataset_fn(mode, params):\n",
    "    # Create a dataset from a geneator\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator=lambda: mnist_generator(mode), \n",
    "        output_types=(tf.float32, tf.int32),\n",
    "        output_shapes=([28, 28, 1], None)\n",
    "    )\n",
    "\n",
    "    # Batch inputs, make sure all batches are equal-sized (drop_remainder=True)\n",
    "    dataset = dataset.batch(params['batch_size'], drop_remainder=True)\n",
    "    # Prefetch some batches\n",
    "    dataset = dataset.prefetch(8)\n",
    "    # Repeat for 100 epochs, leave blank to repeat indefinately\n",
    "    dataset = dataset.repeat(1)\n",
    "    # Return the dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The new way to tell TF not to use all GPUs but only the exact amount needed\n",
    "# The next cell will attempt to use GPU space if executed (even if told to use CPU), so do it now\n",
    "#  otherwise it would be fine to do it just before running the model\n",
    "\n",
    "# Default config does all of this for you\n",
    "default_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errors in the above functions are particularly hard to debug once in the network\n",
    "# you can manually inspect everything works like this\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    dataset = dataset_fn(Mode.TRAIN, params)\n",
    "    for x, y in dataset:\n",
    "        for batch in range(params['batch_size']):\n",
    "            print(y[batch])\n",
    "            plt.imshow(x[batch, ..., 0])\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the experiment\n",
    "\n",
    "Experiments are executed in two steps:\n",
    "\n",
    "1. Experiment discovery: That is, given the experiment name (or model name) find the corresponding saved checkpoint (if any). This process might not be completed immediately due to:\n",
    "  * Remote execution, when the model has to be fetched from the main data-storage\n",
    "  * Local jupyter execution, when the model is selected from the drop-down\n",
    "  \n",
    "2. Experiment execution: Calling train/eval/test and waiting for the results. SenseTheFlow is asynchronous by default, which means that all executions are done in a background thread. Some mechanisms are provided to block until certain events\n",
    "  * `wait_ready` might be used to wait until the experiment actually starts running (waits until tensorflow has setup the GPU memory and starts iterating)\n",
    "  * `wait_for` can be used to wait for a certain number of steps or seconds\n",
    "  * `wait` can be used to block until the experiment completely finishes\n",
    "  \n",
    "Alternatively, asynchronous execution can be turned off by specifying `sync=True` to any of the calls above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As always, you may not want debug messages\n",
    "\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default everything is run in Graph mode (ie. compiled, fast). It can be optionally executed eagerly (step by step) by using\n",
    "#tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "# Supply the name, model CLASS, params and where to write\n",
    "experiment = Experiment(params['model_name'], MNIST, params=params, persistent_path=os.path.join('/tmp/', params['model_name']))\n",
    "# Tell it which GPU to use\n",
    "experiment.assign_gpu(1)\n",
    "# And search for existing models\n",
    "discovery = experiment.run_local(prepend_timestamp=True)\n",
    "\n",
    "# A selection menu will drop down, continuing raise an error unless something is selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two special methods that might be called pre/post initialization of the network\n",
    "#  * Pre-initialization is useful to manually load another checkpoint (aside from the one of this same model,\n",
    "#    which is loaded automatically)\n",
    "#  * Post-initialization is used to load some keras-specific weights, as is the case with \n",
    "#    resnet and such models\n",
    "\n",
    "def pre_initialize_fn(experiment, model, mode, ckpt):\n",
    "    # Also, you can add here some special hooks that, for example, call methods of the model itself\n",
    "    # We reset all metrics (only during mode=Mode.TRAIN!) with the following hook\n",
    "    experiment.add_hook(Hookpoint.LOOP, ExperimentHook('reset', model.params['summary_steps'], model.reset, concurrent=False, mode=Mode.TRAIN))\n",
    "\n",
    "    # You can also trigger evaluations here (TRAIN only!)\n",
    "    # Make sure they are after a checkpoint is done. Currently it requires specifying the same amount of\n",
    "    #  steps. In the future you will be able to do \"experiment.get_hook('checkpoint').steps\"\n",
    "    #experiment.add_hook(Hookpoint.LOOP, ExperimentHook('eval', 1000, lambda *args, **kwargs: experiment.eval(dataset_fn), mode=Mode.TRAIN))\n",
    "    \n",
    "    # We can manually load another chekpoint by manually doing so\n",
    "    # NOTE that SenseTheFlow will still load the latest checkpoint after, an schematic is provided below\n",
    "    \"\"\"\n",
    "    ckpt = tf.train.Checkpoint(net=model)\n",
    "    warm_start = tf.train.latest_checkpoint('/path to some othe model whatsoever')\n",
    "    print('Warm starting from {}'.format(warm_start))\n",
    "    restore = ckpt.restore(warm_start)\n",
    "    return restore, restore.assert_existing_objects_matched # See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint for assert_existing_objects_matched\n",
    "    \"\"\"\n",
    "\n",
    "def post_initialize_fn(experiment, model, mode, last_checkpoint):\n",
    "    # In case the model has an existing checkpoint, it will have already been loaded by now\n",
    "    #  which means Resnet is already loaded\n",
    "    # If we allowed the resnet to be modified it is of high importance to return here, otherwise\n",
    "    #  we will load over the trained weights the original resnet\n",
    "    if last_checkpoint:\n",
    "        print('Skip RESNET load')\n",
    "        return\n",
    "    \n",
    "    # Otherwise, load the weights\n",
    "    print('Loading resnet')\n",
    "    model.resnet.load()\n",
    "\n",
    "# Notify of the selected model\n",
    "experiment.on_discovered(discovery)\n",
    "\n",
    "# Execute eagerly\n",
    "#tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# OPTIMIZER CREATED HERE\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "\n",
    "# Call train, by default will:\n",
    "#  * checkpoint_steps=1000, saves the model every 1000 steps\n",
    "#  * summary_steps=100, saves summaries every 100 steps\n",
    "#  * config=None, which means it will *not* use all gpus, but gradually expand memory\n",
    "# By default the model is executed in a background thread, which might be a BAD idea\n",
    "#  if you have errors and want to find them\n",
    "#  use sync=True to block and run normally\n",
    "context = experiment.train(dataset_fn, optimizer, pre_initialize_fn=pre_initialize_fn, post_initialize_fn=post_initialize_fn)\n",
    "\n",
    "# Wait until it actually is running\n",
    "#context.wait_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# OPTIMIZER CREATED HERE\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "\n",
    "for _ in range(100):\n",
    "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # OPTIMIZER PASSED TO TRAIN FUNCTION (EVAL/TEST can also receive it, but will be ignored)\n",
    "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "    print(\"======================================= TRAIN\")\n",
    "    model = experiment.train(dataset_fn, optimizer, pre_initialize_fn=pre_initialize_fn, post_initialize_fn=post_initialize_fn, checkpoint_steps=None, checkpoint_on_epoch=True, sync=True)\n",
    "    print(\"======================================= EVAL\")\n",
    "    model = experiment.eval(dataset_fn, sync=True)\n",
    "    # model = experiment.eval(dataset_fn, optimizer, sync=True) # Also OK, suppresses warnings for optimizer related (non)-issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waiting\n",
    "\n",
    "An experiment can be waited, in general, by calling any of `wait_ready` or `wait`. As opposed to waiting on a context, waiting on an experiment will wait for all individual runs to end (for example, wait for both train and eval to end)\n",
    "\n",
    "It is encoraged to call `wait_ready` before continuing, to ensure the context is registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.wait_ready() # equivalent to context.wait_ready() above\n",
    "# experiment.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further using the context\n",
    "\n",
    "The training `context` above, inside `once_discovered`, can be recovered with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As it is the first and only call to train, out context is in position [0] of Mode.TRAIN\n",
    "# Further calls would be in next positions, ordered by call order always\n",
    "\n",
    "# If, by whatever reason, you lose track of your context, you can get it again by\n",
    "context = experiment.get_context(Mode.TRAIN)[0]\n",
    "\n",
    "# You can also get eval contexts\n",
    "# last_eval_context = experiment.get_context(Mode.EVAL)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display TQDM vars again\n",
    "context.reattach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can do either of this:\n",
    "\n",
    "# context.save()\n",
    "# context.reattach() # Redraws the TQDM bars\n",
    "# context.stop() # Stops training\n",
    "\n",
    "# Wait until it finishes, can exit waiting by pressing stop or CTRL+C, training won't be affected.\n",
    "# context.wait()\n",
    "\n",
    "context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once a context ends or is stopped, you can retrieve the model with the latest parameters trained\n",
    "model = context.model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also, at any moment, ask for a freshly loaded model from the last checkpoint done\n",
    "last_checkpoint_model = experiment.load_model_from_last_checkpoint(Mode.EVAL)\n",
    "\n",
    "# For instance, you can do\n",
    "with tf.device('/gpu:0'):\n",
    "    for x, y in dataset_fn(Mode.TEST, params):\n",
    "        print(y, last_checkpoint_model(x, training=False, step=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
